# â„ï¸ SnowPipeline Project

A modular, schema-aware ingestion pipeline built with **Python Snowpark** for Snowflake.  
It ingests Avro-formatted user data, applies validation and column normalization, and writes audit-traceable outputs with structured reject tracking and observability throughout.

---

## âš¡ At a Glance

- â„ï¸ **Snowflake-Native Pipeline** â€” Python-based ingestion tailored for Snowflakeâ€™s architecture  
- ğŸ§¬ **Schema-Aware Avro Handling** â€” Validates and aligns data against registry-based configs  
- ğŸ—ï¸ **Modular + Audit-Ready Design** â€” Clear separation of ingestion, transformation, validation, and writeback layers  
- ğŸ–¥ï¸ **CLI Execution** â€” Configurable via command-line args or environment variables  
- ğŸ“ˆ **Observability Built-In** â€” Session tracking, structured logging, reject collection, and dashboard summary outputs

---

## ğŸ¯ Purpose

This project was designed as a hands-on learning exercise to complement my existing experience with native Snowflake toolsâ€”such as Snowpipe, streams and tasks, and dynamic tablesâ€”by building a custom ingestion framework using Python and Snowpark.

Rather than replicating Snowflakeâ€™s managed flows, my goal was to explore the architecture, orchestration, validation patterns, and **observability principles** behind a modular pipeline that feels production-ready.

*Designed to showcase Python engineering skills, Snowflake platform fluency, and architectural awareness in building extensible, observable data pipelines.*


## ğŸ—ï¸ Infrastructure Setup

Before running the pipeline, ensure the required Snowflake resources exist. You can bootstrap them by executing the provided script:

### ğŸ“œ `setup_snowflake_resources.sql`

This script creates:

- `DEMO_DB`: Database to host pipeline tables  
- `EMP_DETAILS_AVRO_CLS`: Target table for ingested records  
- `EMPLOYEE_AVRO_REJECTS`: Table for capturing rejected records 
- `SESSION_AUDIT`: Table for logging Snowflake sessions and user activity   
- `raw_data_stage`: External stage pointing to S3 `snowflakesmpdata` bucket

---

## ğŸ§­ Pipeline Architecture (Simplified)

```
           +-----------------------+
           |     Source File       |
           |  (Avro on S3/Stage)   |
           +-----------------------+
                      |
                      â–¼
           +-----------------------+
           |  Ingestion & Mapping  |   â† column normalization
           +-----------------------+
                      |
                      â–¼
           +-----------------------+
           |   Internal Stage CSV  |   â† intermediate write
           +-----------------------+
                      |
                      â–¼
           +-----------------------+
           |    COPY INTO Target   |   â† final table ingestion
           +-----------------------+
                      |
                      â–¼
           +-----------------------+
           |   Reject Collection   |   â† via VALIDATE(job_id)
           +-----------------------+
```

<sub>ğŸ§± *This pipeline uses a temporary internal stage to write and read intermediate files before final ingestion. This pattern supports row-count verification, retry-safe operations, and separation of transformation from ingestion logicâ€”adding a layer of reliability and observability to the write path.*</sub>

---

## ğŸ“ Project Structure

```
snow_pipelineproject/
â”œâ”€â”€ README.md
â”œâ”€â”€ __init__.py
â”œâ”€â”€ dashboard
â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”œâ”€â”€ pipeline_run_history.json
â”‚   â”œâ”€â”€ pipeline_summary.json
â”œâ”€â”€ environment.yml
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ pipeline.log
â”‚   â”œâ”€â”€ pipeline_v3.log
â”œâ”€â”€ project_structure.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ setup_snowflake_resources.sql
â”œâ”€â”€ snow_pipeline_pkg
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â”œâ”€â”€ connection_details.json
â”‚   â”‚   â”œâ”€â”€ copy_to_snowstg_avro_emp_details_avro_cls.json
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”œâ”€â”€ snowflake_private_key.p8
â”‚   â”œâ”€â”€ ingest
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ avro_loader.py
â”‚   â”œâ”€â”€ pipeline_runner.py
â”‚   â”œâ”€â”€ transform
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema_mapper.py
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”‚   â”œâ”€â”€ connection_loader.py
â”‚   â”‚   â”œâ”€â”€ log_setup.py
â”‚   â”‚   â”œâ”€â”€ snowflake_session.py
â”‚   â”‚   â”œâ”€â”€ validators
â”‚   â”œâ”€â”€ validate
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ quality_checks.py
â”‚   â”œâ”€â”€ writeback
â”‚   â”‚   â”œâ”€â”€ stage_writer.py
â”œâ”€â”€ snow_pipeline_pkg.egg-info
â”‚   â”œâ”€â”€ dependency_links.txt
â”‚   â”œâ”€â”€ top_level.txt
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ test_parse_args.py
â”‚   â”œâ”€â”€ validate
â”‚   â”‚   â”œâ”€â”€ test_column_overlap.py
â”‚   â”‚   â”œâ”€â”€ test_connection_loader.py
â”‚   â”‚   â”œâ”€â”€ test_schema_matching.py
â”œâ”€â”€ tree.py
```
## ğŸš€ Setup Instructions

To get started with the SnowPipeline project, follow these steps:

```bash
# Clone the repository
git clone https://github.com/bcptraining/pysnowflakepipeline.git
cd pysnowflakepipeline

# Install dependencies
pip install -r requirements.txt

# Create required Snowflake infrastructure
snowsql -f setup_snowflake_resources.sql

# Run the pipeline with default configuration
python snow_pipeline_pkg/pipeline_runner.py

---

## âš™ï¸ Pipeline Configuration

### `copy_to_snowstg_avro_emp_details_avro_cls.json`

```jsonc
{
  "database_name": "DEMO_DB",
  "schema_name": "PUBLIC",
  "target_table": "EMP_DETAILS_AVRO_CLS",
  "reject_table": "EMPLOYEE_AVRO_REJECTS",
  "source_location": "@my_s3_stage/Avro_folder/userdata1.avro",
  "source_file_type": "avro",
  "log_file": "logs/pipeline.log",
  "allow_partial_schema": true,
  "validate_target_columns_against_schema": "emp_details_avro_cls@v1.0",
  "target_columns": [
    "REGISTRATION", "USER_ID", "FIRST_NAME", "LAST_NAME", "USER_EMAIL"
  ],
  "map_columns": {
    "REGISTRATION_DTTM": "REGISTRATION",
    "ID": "USER_ID",
    "FIRST_NAME": "FIRST_NAME",
    "LAST_NAME": "LAST_NAME",
    "EMAIL": "USER_EMAIL"
  }
}
```

---

## ğŸ§ª Validation

Located in `validate/quality_checks.py`

- `validate_schema_matches_table()` checks actual columns vs config
- `validate_config_against_schema_ref()` compares config to registry schema
- Registry defined in `schemas.py`:

```python
schema_registry = {
  "emp_details_avro_cls": {
    "v1.0": StructType([...])
  }
}

```

---

## ğŸ§¼ Transformation

Located in `transform/schema_mapper.py`

- `apply_column_mapping()` renames based on config
- `drop_unmapped_columns()` prunes excess columns

---

## ğŸ§Š Snowflake Session Management

Located in `utils/snowflake_session.py`

- `managed_snowflake_session()` handles lifecycle
- Writes audit metadata to `SESSION_AUDIT` table  
  - Session ID, User, Warehouse, Role, Created/Closed timestamps

---

## ğŸ“¤ Writeback & Staging

Located in `writeback/stage_writer.py`

- Writes DataFrame to temp internal stage as CSV
- Reads back for `COPY INTO` retry-safe ingestion
- Collects rejects via Snowflakeâ€™s `VALIDATE()` using query ID
- Saves rejects to configured reject table

---

## ğŸ—ƒï¸ Reject Table Structure

| Field            | Description               |
|------------------|---------------------------|
| `ROW_STATUS`     | Row rejection status  
| `ERROR_MESSAGE`  | Description of error  
| `RAW_ROW`        | Raw content of failed row  
| `TARGET_TABLE`   | Destination table  
| `SCHEMA_REF`     | Schema used for validation  

---

## ğŸ“Š Summary Output

Saved to: `dashboard/pipeline_summary.json`

```jsonc
{
  "start_time": "...",
  "end_time": "...",
  "duration_sec": 1.2,
  "files_loaded": 1,
  "rows_inserted": 542,
  "rows_rejected": 8,
  "query_id": "01af0e7c...",
  "rejects": {
    "count": 8,
    "columns": [...],
    "sample": [...]
  }
}
```

---

## ğŸ›¡ï¸ Environment Validation

Located in `validators/environment_validator.py`

- Ensures presence of:
  - `pandas`
  - `snowflake.connector`
  - `snowflake.snowpark`
- Logs missing dependencies and exits

---

## ğŸ“£ Logging Setup

Located in `log_utils.py`

- `SafeConsoleHandler` prevents Unicode console issues  
- `setup_logger()` supports console + file output  
- Writes logs to configurable path

---

## ğŸ§  Utilities

- `config_loader.py` â€“ loads JSON configs  
- `connection_loader.py` â€“ injects RSA key into Snowflake connection  
- `log_setup.py` â€“ alternate logger initializer used in entry point

---

## ğŸ Running the Pipeline
The pipeline runner supports command-line arguments for customizing execution. Here are the available options:
### â–¶ï¸ Default Execution (from repo root):

```bash
python snow_pipeline_pkg/pipeline_runner.py
```

### âš™ï¸ Override Connection and COPY Configs
You can override both configuration files by providing paths explicitly:

```bash
python snow_pipeline_pkg/pipeline_runner.py \
  --connection-config snow_pipeline_pkg/config/connection_details.json \
  --copy-config snow_pipeline_pkg/config/copy_to_snowstg_avro_emp_details_avro_cls.json
```

### ğŸ› Enable Verbose Logging
Useful for debugging and tracing:

```bash
python snow_pipeline_pkg/pipeline_runner.py --verbose
```

### ğŸ“˜ Available Arguments

| Argument               | Description                                                       | Default                                                                 |
|------------------------|-------------------------------------------------------------------|-------------------------------------------------------------------------|
| `--connection-config`  | Path to Snowflake connection config JSON                          | Env `SNOWFLAKE_CONFIG` or `snow_pipeline_pkg/config/connection_details.json` |
| `--copy-config`        | Path to COPY INTO config JSON                                     | Env `COPY_CONFIG` or `snow_pipeline_pkg/config/copy_to_snowstg_avro_emp_details_avro_cls.json` |
| `--verbose`            | Enables debug-level logging                                       | `False`                                                                 |

> Argument parsing is handled via Python's `argparse`, with optional environment variable support for flexible configuration.


---

## ğŸŒ± Future Enhancements

- YAML support for configs 
- Support additional source file types (beyond AVRO)
- Multi-target orchestration  
- DAG or task scheduler integration  
- Schema diffing and audit mode  
- Dashboard visualizations from summary  

---

## ğŸ“„ License

MIT
