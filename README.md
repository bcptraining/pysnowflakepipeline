# ‚ùÑÔ∏è SnowPipeline Project

A modular, schema-aware ingestion pipeline built for Snowflake that loads Avro-formatted user data into a target table with column normalization, validation, audit logging, and structured reject tracking.

---

## üéØ Purpose

This project was designed as a hands-on learning exercise to complement my existing experience with native Snowflake tools‚Äîsuch as Snowpipe, streams and tasks, and dynamic tables‚Äîby building a modular ingestion pipeline using Python and Snowpark.

Rather than replicating Snowflake‚Äôs managed flows, my goal was to explore the architecture, orchestration, validation patterns, and **observability principles** behind a custom-built ingestion framework:

- üîß **Modular Python Design** ‚Äî Structured with clear separation across ingestion, transformation, validation, and writeback layers.
- üì¶ **Pyproject-Based Packaging** ‚Äî Built as a Python package with dependency management, entry points, and clean directory layout.
- üß¨ **Schema Registry Integration** ‚Äî Implements a versioned registry for config and schema alignment.
- üß™ **Layered Validation Logic** ‚Äî Ensures audit-grade reliability through multiple column checks.
- üßä **Snowflake Session Auditing** ‚Äî Context-managed sessions log usage metadata to `SESSION_AUDIT`.
- üìà **Observability Features** ‚Äî Structured logging, reject tracking, and summary outputs for full pipeline visibility.
- üñ•Ô∏è **CLI-Compatible Config Loading** ‚Äî Pipeline runner supports configuration overrides via command-line arguments or environment variables.

> ‚ö†Ô∏è **Note on Transformations**  
> Transformations are intentionally minimal‚Äîlimited to column projection and renaming‚Äîsince the use case focused on staging conformed data for downstream consumption. This keeps the pipeline focused, auditable, and extensible without unnecessary complexity.

This project showcases Python engineering skills, Snowflake platform fluency, and architectural awareness when designing extensible, observable data pipelines.  

## üèóÔ∏è Infrastructure Setup

Before running the pipeline, ensure the required Snowflake resources exist. You can bootstrap them by executing the provided script:

### üìú `setup_snowflake_resources.sql`

This script creates:

- `DEMO_DB`: Database to host pipeline tables  
- `EMP_DETAILS_AVRO_CLS`: Target table for ingested records  
- `EMPLOYEE_AVRO_REJECTS`: Table for capturing rejected records 
- `SESSION_AUDIT`: Table for logging Snowflake sessions and user activity   
- `raw_data_stage`: External stage pointing to S3 `snowflakesmpdata` bucket

---

## üß≠ Pipeline Architecture (Simplified)

```
           +-----------------------+
           |     Source File       |
           |  (Avro on S3/Stage)   |
           +-----------------------+
                      |
                      ‚ñº
           +-----------------------+
           |  Ingestion & Mapping  |   ‚Üê column normalization
           +-----------------------+
                      |
                      ‚ñº
           +-----------------------+
           |   Internal Stage CSV  |   ‚Üê intermediate write
           +-----------------------+
                      |
                      ‚ñº
           +-----------------------+
           |    COPY INTO Target   |   ‚Üê final table ingestion
           +-----------------------+
                      |
                      ‚ñº
           +-----------------------+
           |   Reject Collection   |   ‚Üê via VALIDATE(job_id)
           +-----------------------+
```

<sub>üß± *This pipeline uses a temporary internal stage to write and read intermediate files before final ingestion. This pattern supports row-count verification, retry-safe operations, and separation of transformation from ingestion logic‚Äîadding a layer of reliability and observability to the write path.*</sub>

---

## üìÅ Project Structure

```
snow_pipelineproject/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ dashboard
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_run_history.json
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_summary.json
‚îú‚îÄ‚îÄ environment.yml
‚îú‚îÄ‚îÄ logs
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.log
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_v3.log
‚îú‚îÄ‚îÄ project_structure.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ setup_snowflake_resources.sql
‚îú‚îÄ‚îÄ snow_pipeline_pkg
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ __main__.py
‚îÇ   ‚îú‚îÄ‚îÄ config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection_details.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ copy_to_snowstg_avro_emp_details_avro_cls.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ snowflake_private_key.p8
‚îÇ   ‚îú‚îÄ‚îÄ ingest
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ avro_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_runner.py
‚îÇ   ‚îú‚îÄ‚îÄ transform
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_mapper.py
‚îÇ   ‚îú‚îÄ‚îÄ utils
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log_setup.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ snowflake_session.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators
‚îÇ   ‚îú‚îÄ‚îÄ validate
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality_checks.py
‚îÇ   ‚îú‚îÄ‚îÄ writeback
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stage_writer.py
‚îú‚îÄ‚îÄ snow_pipeline_pkg.egg-info
‚îÇ   ‚îú‚îÄ‚îÄ dependency_links.txt
‚îÇ   ‚îú‚îÄ‚îÄ top_level.txt
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_parse_args.py
‚îÇ   ‚îú‚îÄ‚îÄ validate
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_column_overlap.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_connection_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_schema_matching.py
‚îú‚îÄ‚îÄ tree.py
```
---

## ‚öôÔ∏è Pipeline Configuration

### `copy_to_snowstg_avro_emp_details_avro_cls.json`

```jsonc
{
  "database_name": "DEMO_DB",
  "schema_name": "PUBLIC",
  "target_table": "EMP_DETAILS_AVRO_CLS",
  "reject_table": "EMPLOYEE_AVRO_REJECTS",
  "source_location": "@my_s3_stage/Avro_folder/userdata1.avro",
  "source_file_type": "avro",
  "log_file": "logs/pipeline.log",
  "allow_partial_schema": true,
  "validate_target_columns_against_schema": "emp_details_avro_cls@v1.0",
  "target_columns": [
    "REGISTRATION", "USER_ID", "FIRST_NAME", "LAST_NAME", "USER_EMAIL"
  ],
  "map_columns": {
    "REGISTRATION_DTTM": "REGISTRATION",
    "ID": "USER_ID",
    "FIRST_NAME": "FIRST_NAME",
    "LAST_NAME": "LAST_NAME",
    "EMAIL": "USER_EMAIL"
  }
}
```

---

## üß™ Validation

Located in `validate/quality_checks.py`

- `validate_schema_matches_table()` checks actual columns vs config
- `validate_config_against_schema_ref()` compares config to registry schema
- Registry defined in `schemas.py`:

```python
schema_registry = {
  "emp_details_avro_cls": {
    "v1.0": StructType([...])
  }
}

```

---

## üßº Transformation

Located in `transform/schema_mapper.py`

- `apply_column_mapping()` renames based on config
- `drop_unmapped_columns()` prunes excess columns

---

## üßä Snowflake Session Management

Located in `utils/snowflake_session.py`

- `managed_snowflake_session()` handles lifecycle
- Writes audit metadata to `SESSION_AUDIT` table  
  - Session ID, User, Warehouse, Role, Created/Closed timestamps

---

## üì§ Writeback & Staging

Located in `writeback/stage_writer.py`

- Writes DataFrame to temp internal stage as CSV
- Reads back for `COPY INTO` retry-safe ingestion
- Collects rejects via Snowflake‚Äôs `VALIDATE()` using query ID
- Saves rejects to configured reject table

---

## üóÉÔ∏è Reject Table Structure

| Field            | Description               |
|------------------|---------------------------|
| `ROW_STATUS`     | Row rejection status  
| `ERROR_MESSAGE`  | Description of error  
| `RAW_ROW`        | Raw content of failed row  
| `TARGET_TABLE`   | Destination table  
| `SCHEMA_REF`     | Schema used for validation  

---

## üìä Summary Output

Saved to: `dashboard/pipeline_summary.json`

```jsonc
{
  "start_time": "...",
  "end_time": "...",
  "duration_sec": 1.2,
  "files_loaded": 1,
  "rows_inserted": 542,
  "rows_rejected": 8,
  "query_id": "01af0e7c...",
  "rejects": {
    "count": 8,
    "columns": [...],
    "sample": [...]
  }
}
```

---

## üõ°Ô∏è Environment Validation

Located in `validators/environment_validator.py`

- Ensures presence of:
  - `pandas`
  - `snowflake.connector`
  - `snowflake.snowpark`
- Logs missing dependencies and exits

---

## üì£ Logging Setup

Located in `log_utils.py`

- `SafeConsoleHandler` prevents Unicode console issues  
- `setup_logger()` supports console + file output  
- Writes logs to configurable path

---

## üß† Utilities

- `config_loader.py` ‚Äì loads JSON configs  
- `connection_loader.py` ‚Äì injects RSA key into Snowflake connection  
- `log_setup.py` ‚Äì alternate logger initializer used in entry point

---

## üèÅ Running the Pipeline
The pipeline runner supports command-line arguments for customizing execution. Here are the available options:
### ‚ñ∂Ô∏è Default Execution (from repo root):

```bash
python snow_pipeline_pkg/pipeline_runner.py
```

### ‚öôÔ∏è Override Connection and COPY Configs
You can override both configuration files by providing paths explicitly:

```bash
python snow_pipeline_pkg/pipeline_runner.py \
  --connection-config snow_pipeline_pkg/config/connection_details.json \
  --copy-config snow_pipeline_pkg/config/copy_to_snowstg_avro_emp_details_avro_cls.json
```

### üêõ Enable Verbose Logging
Useful for debugging and tracing:

```bash
python snow_pipeline_pkg/pipeline_runner.py --verbose
```

### üìò Available Arguments

| Argument               | Description                                                       | Default                                                                 |
|------------------------|-------------------------------------------------------------------|-------------------------------------------------------------------------|
| `--connection-config`  | Path to Snowflake connection config JSON                          | Env `SNOWFLAKE_CONFIG` or `snow_pipeline_pkg/config/connection_details.json` |
| `--copy-config`        | Path to COPY INTO config JSON                                     | Env `COPY_CONFIG` or `snow_pipeline_pkg/config/copy_to_snowstg_avro_emp_details_avro_cls.json` |
| `--verbose`            | Enables debug-level logging                                       | `False`                                                                 |

> Argument parsing is handled via Python's `argparse`, with optional environment variable support for flexible configuration.


---

## üå± Future Enhancements

- YAML support for configs 
- Support additional source file types (beyond AVRO)
- Multi-target orchestration  
- DAG or task scheduler integration  
- Schema diffing and audit mode  
- Dashboard visualizations from summary  

---

## üìÑ License

MIT
